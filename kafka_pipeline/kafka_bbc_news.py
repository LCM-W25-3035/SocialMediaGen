# -*- coding: utf-8 -*-
"""Kafka_BBC_News.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Uxyb7BLOuTIFlXWwZKtdPA7RZyHYrqY
"""

!pip install requests beautifulsoup4
!pip install pymongo

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
import datetime

# Function to connect to MongoDB
def connect_mongo():
    try:
        client = MongoClient("mongodb+srv://Muskan:MUSKAN25@newsanalytics.rzmco.mongodb.net/?retryWrites=true&w=majority&appName=NEWSANALYTICS")
        db = client["news_database"]
        collection = db["bbc_headlines"]

        # Remove duplicate entries with empty or duplicate links before creating the index
        print("Cleaning up duplicate entries...")
        collection.delete_many({"link": ""})  # Remove records with empty links
        print("Removed empty links.")

        # Ensure unique index on the 'link' field
        collection.create_index("link", unique=True)
        print("Unique index created on 'link' field.")

        return collection
    except Exception as e:
        print("Error connecting to MongoDB:", e)
        return None

# Function to scrape description from individual article
def scrape_article_description(link):
    try:
        response = requests.get(link)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # Extract description from meta tag
            description_tag = soup.find('meta', attrs={"name": "description"})
            if description_tag and description_tag.get("content"):
                return description_tag["content"]
            else:
                return "No description available."
        else:
            return "Failed to fetch article."
    except Exception as e:
        print(f"Error scraping description for {link}: {e}")
        return "Error fetching description."

# Function to scrape BBC News headlines
def scrape_bbc_news():
    url = "https://www.bbc.com/news"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Find all headlines
            headlines = soup.select('h2')

            if not headlines:
                print("No headlines found. The page structure may have changed.")
                return []

            news_list = []
            for headline in headlines:
                title = headline.get_text(strip=True)
                link_tag = headline.find_parent('a')  # Get the parent anchor tag for link
                link = link_tag['href'] if link_tag else ''
                full_link = f"https://www.bbc.com{link}" if link.startswith('/') else link

                # Skip empty links
                if full_link.strip():
                    description = scrape_article_description(full_link)

                    news_list.append({
                        "title": title,
                        "link": full_link,
                        "description": description,
                        "scraped_at": datetime.datetime.now().isoformat()
                    })

            return news_list
        else:
            print(f"Failed to retrieve BBC News. Status code: {response.status_code}")
            return []

    except Exception as e:
        print("Error occurred while scraping BBC News:", e)
        return []

# Function to store news in MongoDB with deduplication
def store_in_mongo(news_data):
    collection = connect_mongo()
    if collection is not None:
        inserted_count = 0
        for news in news_data:
            try:
                collection.insert_one(news)
                inserted_count += 1
            except errors.DuplicateKeyError:
                print(f"Duplicate entry found, skipping: {news['title']}")

        print(f"Inserted {inserted_count} new articles into MongoDB.")
    else:
        print("Failed to connect to MongoDB.")

# Main function to run the scraper and store data
if __name__ == "__main__":
    print("Starting BBC News scraping...")
    news_data = scrape_bbc_news()

    if news_data:
        for news in news_data:
            print(f"Title: {news['title']}\nLink: {news['link']}\nDescription: {news['description']}\n")

        store_in_mongo(news_data)
    else:
        print("No news articles scraped.")

pip install kafka-python

import requests
from bs4 import BeautifulSoup
import datetime
from kafka import KafkaProducer, KafkaConsumer
import json
from pymongo import MongoClient

# Function to produce data to Kafka topic
def produce_to_kafka(news_data):
    producer = KafkaProducer(
        bootstrap_servers='localhost:9092',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    for news in news_data:
        producer.send('bbcnews_topic', value=news)
        print(f"Produced to Kafka: {news['title']}")
    producer.flush()

# Function to consume data from Kafka and store it in MongoDB
def consume_and_store():
    consumer = KafkaConsumer(
        'bbcnews_topic',
        bootstrap_servers='localhost:9092',
        auto_offset_reset='earliest',
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    collection = connect_mongo()
    if collection:
        for message in consumer:
            news = message.value
            try:
                collection.insert_one(news)
                print(f"Stored in MongoDB: {news['title']}")
            except Exception as e:
                print(f"Error inserting to MongoDB: {e}")

