{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd773c73-43f3-4b08-ae6b-ce6d925a3bc1",
   "metadata": {},
   "source": [
    "This code is leveraged by Chatgpt and the prompt is given below\n",
    "1.I only want trending news and nothing else which can help me analyse the trend by which news is the most trending recently or which one is the most recent like that\n",
    "2.I have multiple json files which contains news can you make the same code for them files\n",
    "3.The title I am getting is nan so I have to get a proper headline detail can you help me with that\n",
    "This code relies on frequency analysis and sorting to determine trending news.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39293f70-e73d-4e70-87ef-535b6e0043be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ Available columns in the dataset: ['_id', 'headline', 'summary', 'link', 'timestamp', 'title', 'Summary', 'category']\n",
      "\n",
      "ğŸ”¥ Most Mentioned Keywords in Headlines:\n",
      "[('to', 777), ('in', 772), ('of', 485), ('the', 416), ('and', 310), ('on', 310), ('a', 310), ('as', 283), ('for', 280), ('says', 268)]\n",
      "\n",
      "ğŸ”¹ Top Trending News Articles (Most Recent First):\n",
      "                                            final_title  \\\n",
      "6416  Pennsylvania's governor says the Trump adminis...   \n",
      "6415  US says OKX crypto exchange operator enters $5...   \n",
      "6414  â€˜Devastatingâ€™: Mass. school district mourning ...   \n",
      "6413  Intel says new ASML machines are in production...   \n",
      "6412  Judge upholds White House ban on AP amid Gulf ...   \n",
      "6411  Who is Dan Bongino, the new deputy director of...   \n",
      "928   Leeds stun Sheff Utd with comeback to go five ...   \n",
      "927   Ex-surgeon admits 'despicable acts' in French ...   \n",
      "926   Federal worker unions sue Musk over demand the...   \n",
      "925   Peace must not mean surrendering Ukraine, Macr...   \n",
      "\n",
      "                 published_date  \n",
      "6416 2025-02-25 00:02:11.643551  \n",
      "6415 2025-02-25 00:02:11.567134  \n",
      "6414 2025-02-25 00:02:11.490640  \n",
      "6413 2025-02-25 00:02:11.184629  \n",
      "6412 2025-02-25 00:02:10.955366  \n",
      "6411 2025-02-25 00:02:10.726166  \n",
      "928  2025-02-25 00:02:02.075929  \n",
      "927  2025-02-25 00:02:01.653583  \n",
      "926  2025-02-25 00:02:01.181455  \n",
      "925  2025-02-25 00:02:00.703437  \n"
     ]
    }
   ],
   "source": [
    "import os  # For working with files and directories\n",
    "import json  # For reading JSON files\n",
    "import pandas as pd  # For handling tabular data\n",
    "from collections import Counter  # For counting occurrences of words\n",
    "\n",
    "# ğŸ“‚ Define the folder path where JSON files are stored\n",
    "folder_path = r\"C:\\Users\\User\\Desktop\\second year\\Big Data Capstone - Bhavik\\NewsData Download Database\"\n",
    "\n",
    "# ğŸ“‹ Initialize an empty list to store news articles from all JSON files\n",
    "news_list = []\n",
    "\n",
    "# ğŸ”„ Loop through all files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".json\"):  # âœ… Process only JSON files\n",
    "        file_path = os.path.join(folder_path, file)  # ğŸ“Œ Get the full file path\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:  # ğŸ“– Open and read the JSON file\n",
    "            data = json.load(f)  # ğŸ”„ Load the JSON content\n",
    "            \n",
    "            # âœ… Check if the JSON file contains a list of news articles\n",
    "            if isinstance(data, list):\n",
    "                news_list.extend(data)  # ğŸ“¥ Append all articles to the main list\n",
    "            \n",
    "            # âœ… If the JSON file contains a single news article as a dictionary\n",
    "            elif isinstance(data, dict):\n",
    "                news_list.append(data)  # ğŸ“¥ Convert it to a list and append\n",
    "\n",
    "# ğŸ— Convert the list of news articles into a Pandas DataFrame\n",
    "df = pd.DataFrame(news_list)\n",
    "\n",
    "# ğŸ›  Debugging: Print column names to check what exists in the dataset\n",
    "print(\"\\nğŸ“Œ Available columns in the dataset:\", df.columns.tolist())\n",
    "\n",
    "# ğŸ” Define possible names for the publication date column (varies across datasets)\n",
    "possible_date_columns = [\"published_date\", \"timestamp\", \"date\", \"pub_date\"]\n",
    "\n",
    "# ğŸ”„ Loop through the list of possible date column names\n",
    "for col in possible_date_columns:\n",
    "    if col in df.columns:  # âœ… If the column exists in the dataset\n",
    "        df.rename(columns={col: \"published_date\"}, inplace=True)  # ğŸ”„ Standardize its name\n",
    "        break  # â¹ Stop checking once a match is found\n",
    "\n",
    "# âš ï¸ If no valid date column is found, print an error and exit the script\n",
    "else:\n",
    "    print(\"âŒ No valid 'published_date' column found! Check your JSON structure.\")\n",
    "    print(df.head())  # ğŸ” Print the first few rows for debugging\n",
    "    exit()  # ğŸš¨ Stop execution since the date column is necessary\n",
    "\n",
    "# ğŸ•’ Convert 'published_date' column to datetime format (handling errors gracefully)\n",
    "df[\"published_date\"] = pd.to_datetime(df[\"published_date\"], errors=\"coerce\")\n",
    "\n",
    "# ğŸ—‘ Remove rows where 'published_date' could not be converted (invalid/missing dates)\n",
    "df = df.dropna(subset=[\"published_date\"])\n",
    "\n",
    "# ğŸ”½ Sort articles by 'published_date' in descending order (most recent first)\n",
    "df = df.sort_values(by=\"published_date\", ascending=False)\n",
    "\n",
    "# ğŸ” Check if the dataset contains 'title' column; otherwise, use 'headline'\n",
    "if \"title\" in df.columns and df[\"title\"].notna().sum() > 0:\n",
    "    df[\"final_title\"] = df[\"title\"]  # âœ… Use 'title' if available\n",
    "elif \"headline\" in df.columns:\n",
    "    df[\"final_title\"] = df[\"headline\"]  # âœ… Use 'headline' if 'title' is missing\n",
    "else:\n",
    "    print(\"âŒ No 'title' or 'headline' column found in the dataset.\")  # ğŸš¨ Error message\n",
    "    exit()  # â¹ Stop execution since a title is required\n",
    "\n",
    "# ğŸ”¡ Convert all titles to lowercase and split into individual words (for keyword analysis)\n",
    "df[\"title_keywords\"] = df[\"final_title\"].str.lower().str.split()\n",
    "\n",
    "# ğŸ”¢ Flatten the list of words from all news titles and count word occurrences\n",
    "all_keywords = [word for words in df[\"title_keywords\"].dropna() for word in words]\n",
    "trending_keywords = Counter(all_keywords).most_common(10)  # ğŸ”¥ Find the top 10 most common words\n",
    "\n",
    "# ğŸ“Š Print the most mentioned words (trending keywords)\n",
    "print(\"\\nğŸ”¥ Most Mentioned Keywords in Headlines:\")\n",
    "print(trending_keywords)\n",
    "\n",
    "# ğŸ“° Print the top 10 most recent trending news articles\n",
    "print(\"\\nğŸ”¹ Top Trending News Articles (Most Recent First):\")\n",
    "print(df[[\"final_title\", \"published_date\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124d1d0-dac2-4c6f-86f3-322e08b073e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df09ea7-dd17-4165-91bc-2a427709d6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
